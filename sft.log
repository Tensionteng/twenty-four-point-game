ipex flag is deprecated, will be removed in Accelerate v1.10. From 2.7.0, PyTorch has all needed optimizations for Intel CPU and XPU.
--- [1/4] 正在加载模型和分词器 ---
--- [1/4] 正在加载模型和分词器 ---
--- [1/4] 正在加载模型和分词器 ---
--- [1/4] 正在加载模型和分词器 ---
--- [2/4] 正在加载并准备数据集 ---
数据集加载成功:
DatasetDict({
    train: Dataset({
        features: ['messages'],
        num_rows: 9000
    })
    validation: Dataset({
        features: ['messages'],
        num_rows: 500
    })
    test: Dataset({
        features: ['messages'],
        num_rows: 500
    })
})
--- [3/4] 正在初始化SFTTrainer ------ [2/4] 正在加载并准备数据集 ---

/home/tengshiyuan/code/twenty-four/.venv/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank2]:[W807 05:23:08.012107540 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
数据集加载成功:
DatasetDict({
    train: Dataset({
        features: ['messages'],
        num_rows: 9000
    })
    validation: Dataset({
        features: ['messages'],
        num_rows: 500
    })
    test: Dataset({
        features: ['messages'],
        num_rows: 500
    })
})
--- [3/4] 正在初始化SFTTrainer ---
/home/tengshiyuan/code/twenty-four/.venv/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank3]:[W807 05:23:08.024339819 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
--- [2/4] 正在加载并准备数据集 ---
数据集加载成功:
DatasetDict({
    train: Dataset({
        features: ['messages'],
        num_rows: 9000
    })
    validation: Dataset({
        features: ['messages'],
        num_rows: 500
    })
    test: Dataset({
        features: ['messages'],
        num_rows: 500
    })
})
--- [3/4] 正在初始化SFTTrainer ---
/home/tengshiyuan/code/twenty-four/.venv/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank1]:[W807 05:23:08.154322712 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
--- [2/4] 正在加载并准备数据集 ---
数据集加载成功:
DatasetDict({
    train: Dataset({
        features: ['messages'],
        num_rows: 9000
    })
    validation: Dataset({
        features: ['messages'],
        num_rows: 500
    })
    test: Dataset({
        features: ['messages'],
        num_rows: 500
    })
})
--- [3/4] 正在初始化SFTTrainer ---
/home/tengshiyuan/code/twenty-four/.venv/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank0]:[W807 05:23:09.328655826 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/tengshiyuan/code/twenty-four/train.py", line 117, in <module>
[rank3]:     main()
[rank3]:   File "/home/tengshiyuan/code/twenty-four/train.py", line 88, in main
[rank3]:     trainer = SFTTrainer(
[rank3]:               ^^^^^^^^^^^
[rank3]:   File "/home/tengshiyuan/code/twenty-four/.venv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py", line 519, in __init__
[rank3]:     train_dataset = self._prepare_dataset(
[rank3]:                     ^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/tengshiyuan/code/twenty-four/.venv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py", line 684, in _prepare_dataset
[rank3]:     with PartialState().main_process_first():
[rank3]:   File "/home/tengshiyuan/.local/share/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/contextlib.py", line 137, in __enter__
[rank3]:     return next(self.gen)
[rank3]:            ^^^^^^^^^^^^^^
[rank3]:   File "/home/tengshiyuan/code/twenty-four/.venv/lib/python3.11/site-packages/accelerate/state.py", line 526, in main_process_first
[rank3]:     yield from self._goes_first(self.is_main_process)
[rank3]:   File "/home/tengshiyuan/code/twenty-four/.venv/lib/python3.11/site-packages/accelerate/state.py", line 409, in _goes_first
[rank3]:     self.wait_for_everyone()
[rank3]:   File "/home/tengshiyuan/code/twenty-four/.venv/lib/python3.11/site-packages/accelerate/state.py", line 403, in wait_for_everyone
[rank3]:     torch.distributed.barrier()
[rank3]:   File "/home/tengshiyuan/code/twenty-four/.venv/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
[rank3]:     return func(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/tengshiyuan/code/twenty-four/.venv/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 4635, in barrier
[rank3]:     work = group.barrier(opts=opts)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]: torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:3356, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.26.2
[rank3]: ncclUnhandledCudaError: Call to CUDA function failed.
[rank3]: Last error:
[rank3]: Cuda failure 2 'out of memory'
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/tengshiyuan/code/twenty-four/train.py", line 117, in <module>
[rank0]:     main()
[rank0]:   File "/home/tengshiyuan/code/twenty-four/train.py", line 88, in main
[rank0]:     trainer = SFTTrainer(
[rank0]:               ^^^^^^^^^^^
[rank0]:   File "/home/tengshiyuan/code/twenty-four/.venv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py", line 519, in __init__
[rank0]:     train_dataset = self._prepare_dataset(
[rank0]:                     ^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/tengshiyuan/code/twenty-four/.venv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py", line 684, in _prepare_dataset
[rank0]:     with PartialState().main_process_first():
[rank0]:   File "/home/tengshiyuan/.local/share/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/contextlib.py", line 144, in __exit__
[rank0]:     next(self.gen)
[rank0]:   File "/home/tengshiyuan/code/twenty-four/.venv/lib/python3.11/site-packages/accelerate/state.py", line 526, in main_process_first
[rank0]:     yield from self._goes_first(self.is_main_process)
[rank0]:   File "/home/tengshiyuan/code/twenty-four/.venv/lib/python3.11/site-packages/accelerate/state.py", line 414, in _goes_first
[rank0]:     self.wait_for_everyone()
[rank0]:   File "/home/tengshiyuan/code/twenty-four/.venv/lib/python3.11/site-packages/accelerate/state.py", line 403, in wait_for_everyone
[rank0]:     torch.distributed.barrier()
[rank0]:   File "/home/tengshiyuan/code/twenty-four/.venv/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/tengshiyuan/code/twenty-four/.venv/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 4635, in barrier
[rank0]:     work = group.barrier(opts=opts)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:3356, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.26.2
[rank0]: ncclUnhandledCudaError: Call to CUDA function failed.
[rank0]: Last error:
[rank0]: Cuda failure 2 'out of memory'
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/tengshiyuan/code/twenty-four/train.py", line 117, in <module>
[rank1]:     main()
[rank1]:   File "/home/tengshiyuan/code/twenty-four/train.py", line 88, in main
[rank1]:     trainer = SFTTrainer(
[rank1]:               ^^^^^^^^^^^
[rank1]:   File "/home/tengshiyuan/code/twenty-four/.venv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py", line 519, in __init__
[rank1]:     train_dataset = self._prepare_dataset(
[rank1]:                     ^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/tengshiyuan/code/twenty-four/.venv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py", line 684, in _prepare_dataset
[rank1]:     with PartialState().main_process_first():
[rank1]:   File "/home/tengshiyuan/.local/share/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/contextlib.py", line 137, in __enter__
[rank1]:     return next(self.gen)
[rank1]:            ^^^^^^^^^^^^^^
[rank1]:   File "/home/tengshiyuan/code/twenty-four/.venv/lib/python3.11/site-packages/accelerate/state.py", line 526, in main_process_first
[rank1]:     yield from self._goes_first(self.is_main_process)
[rank1]:   File "/home/tengshiyuan/code/twenty-four/.venv/lib/python3.11/site-packages/accelerate/state.py", line 409, in _goes_first
[rank1]:     self.wait_for_everyone()
[rank1]:   File "/home/tengshiyuan/code/twenty-four/.venv/lib/python3.11/site-packages/accelerate/state.py", line 403, in wait_for_everyone
[rank1]:     torch.distributed.barrier()
[rank1]:   File "/home/tengshiyuan/code/twenty-four/.venv/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
[rank1]:     return func(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/tengshiyuan/code/twenty-four/.venv/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 4635, in barrier
[rank1]:     work = group.barrier(opts=opts)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]: torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:3356, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.26.2
[rank1]: ncclUnhandledCudaError: Call to CUDA function failed.
[rank1]: Last error:
[rank1]: Cuda failure 2 'out of memory'
[rank0]:[W807 05:23:10.185527881 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
W0807 05:23:11.048000 2837317 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2837478 closing signal SIGTERM
W0807 05:23:11.049000 2837317 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2837479 closing signal SIGTERM
W0807 05:23:11.050000 2837317 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2837480 closing signal SIGTERM
E0807 05:23:11.829000 2837317 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 2837477) of binary: /home/tengshiyuan/code/twenty-four/.venv/bin/python3
Traceback (most recent call last):
  File "/home/tengshiyuan/code/twenty-four/.venv/bin/accelerate", line 10, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/tengshiyuan/code/twenty-four/.venv/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/home/tengshiyuan/code/twenty-four/.venv/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1190, in launch_command
    multi_gpu_launcher(args)
  File "/home/tengshiyuan/code/twenty-four/.venv/lib/python3.11/site-packages/accelerate/commands/launch.py", line 815, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/tengshiyuan/code/twenty-four/.venv/lib/python3.11/site-packages/torch/distributed/run.py", line 883, in run
    elastic_launch(
  File "/home/tengshiyuan/code/twenty-four/.venv/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 139, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tengshiyuan/code/twenty-four/.venv/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 270, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-07_05:23:11
  host      : gpu8-labot
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2837477)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
